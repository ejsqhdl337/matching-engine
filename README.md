# PoC Matching Engine

This project is a Proof-of-Concept (PoC) matching engine. The primary goal of this project is not to develop a production-ready system, but rather to serve as an exploration ground for architecting complex systems under various challenging requirements. It aims to foster deeper understanding and uncover novel viewpoints that can be applied to future work, particularly in domains requiring high scalability and resilience, such as Web3 applications.

The implementation will leverage Go, chosen for its simplicity, productivity, and emphasis on clear code. In a system as critical and complex as a matching engine, code clarity is paramount to minimizing errors, as even minor confusion can lead to significant issues. The matching engine component must inherently be error-free within the broader system.

[Korean version](README_ko.md)

## Core Architectural Principles & Ideas

### Event-Driven Architecture and Data Flow

The matching engine employs an event-driven architecture to ensure high throughput and low latency. The overall data flow can be visualized as follows:

```+-------------------+     +-------------------+     +-------------------+
|   Input Event     |     |  Matching Engine  |     |   Output Event    |
|       Flow        |     |   (Order Book)    |     |       Flow        |
+-------------------+     +-------------------+     +-------------------+
| Event ring buffer | --- consume --->  | bids              | --- produce --->  | Event ring buffer |
|-------------------|                     |-------------------|                     |-------------------|
| Event handler     |                     | asks              |                     | Event handler     |
+-------------------+                     +-------------------+                     +-------------------+
        |                                                                                |
        |                                                                                |
        +-----------------------------> +-----------------+ <----------------------------+
                                        |   Event Logger  |
                                        +-----------------+
                                          (Responsible for logging, saving to RDBMS/file)

Output Event Types:
- Order Book Snapshots
- Trade
- MarketPrice
```

**Components:**

*   **Input Event Flow:**
    *   **Event Ring Buffer (Input):** A high-performance, low-latency queue that buffers incoming events, such as new orders, modifications, or cancellations, before they are processed by the matching engine.
    *   **Event Handler (Input):** Consumes events from the input ring buffer, performing initial validation and preparing them for the core matching logic.
*   **Matching Engine (Order Book):**
    *   The core component responsible for processing events. It contains the central `Order Book`, which maintains the current state of `bids` (buy orders) and `asks` (sell orders).
*   **Output Event Flow:**
    *   **Event Ring Buffer (Output):** Buffers the results generated by the matching engine (e.g., confirmed trades, order book updates) before they are dispatched to downstream systems.
    *   **Event Handler (Output):** Produces various types of events from the output ring buffer, such as:
        *   **Order Book Snapshots:** Periodic or event-driven captures of the order book's state.
        *   **Trade:** Details of executed matches between bids and asks.
        *   **MarketPrice:** Updates on the current market price derived from trades or order book activity.
*   **Event Logger:**
    *   This component has the critical responsibility of ensuring all significant events are logged. It receives input from both the input and output event handlers, ensuring comprehensive logging of received orders, processed events, and generated outcomes. The persistence mechanism (e.g., RDBMS, file system) for these logs will be determined based on specific durability and retrieval requirements.

### Functional Scope

The matching engine server will exclusively focus on the core logic of matching orders. It will not incorporate business logic unrelated to order matching. However, it will support a flexible "afterOrder" handler function. This mechanism allows for the concurrent execution of additional processes, such as updating state changes to a PostgreSQL database for persistence, without blocking the core matching process. Each order will include an `orderer_id` to uniquely identify the participant, facilitating integration with external user management or accounting systems.

### Transaction Sequencing & Concurrency

A fundamental constraint of order matching is its inherent sequential nature. Each incoming transaction (order) often has a dependency on the state resulting from previous transactions. Altering the order of processing would directly change the outcome of the matching engine. Consequently, the core matching process cannot be truly parallelized in the traditional sense, as transaction order must be strictly preserved.

However, specific scenarios where limited parallel execution might be feasible will be explored. For instance, certain "cancel" orders that affect price ranges where no active matching is occurring (i.e., not requiring mutex locks on critical, actively traded order book levels) could potentially be processed in parallel. This is a brief idea that needs careful consideration to ensure it does not introduce undue complexity for minimal gain, given that such "out-of-range" order cancellations represent a small fraction of overall trading activity.

### System Configuration

The matching engine will support configurable parameters, such as a `minimum_tick` size, which defines the smallest allowable price increment and helps prevent orders on infinitely small price differences.

### Scalability Approach

While the initial design aims for a single logical matching engine, the architecture will allow for horizontal scaling by dedicating separate matching engine instances to specific assets or asset groups that exhibit consistently heavy order flow, thereby preventing bottlenecks for other assets.

### Order Types

Beyond standard market, limit, and stop-loss orders, the engine will explore support for more specialized order types crucial for diverse trading strategies:

*   **Post-Only Orders:** Designed to add liquidity to the order book. A Post-Only order will only execute if it does not immediately match with an existing order. If it would "cross the spread" and act as a market taker, it will be rejected. This type is valuable for liquidity providers aiming to earn maker rebates by avoiding taker fees. [1]
*   **All-or-None (AON):** An order that must be executed in its entirety, or not at all.
*   **Fill-or-Kill (FOK):** An order that must be immediately executed in its entirety, or be cancelled.
*   **Immediate-or-Cancel (IOC):** An order that must be immediately executed (partially or fully), with any unexecuted portion cancelled. [1]

### Testing Framework

A robust testing framework is critical for validating the matching engine's performance and correctness. This framework will:

*   Enable benchmarking of processing speed against existing open-source matching engine solutions.
*   Facilitate comprehensive testing under various special scenarios. This will involve exploring diverse trading scenarios and generating specific test data sets to ensure the project's capabilities are thoroughly verified.

### Insights from Blockchain Architectures

The operational characteristics of blockchain platforms, particularly Ethereum, offer valuable insights for a matching engine. The reasons platforms like OKX have opted to build their own blockchains (e.g., from Ethereum's codebase) resonate with matching engine requirements:

*   **Wallet and Authentication Systems:** Inspiration for robust client interaction and state management.
*   **Sequential Transaction Processing:** The inherent ordered nature of blockchain transactions aligns with the need for strict transaction sequencing and deterministic outcomes in a matching engine.
*   **24/7 No-Downtime Operation:** Blockchain networks are designed for continuous availability, a critical requirement for cryptocurrency exchanges.
*   **Guaranteed Transaction Propagation:** Valid transactions, once broadcast, are designed to propagate and eventually be confirmed, preventing "missing" user transactions.

### In-Memory State Management (Inspired by LMAX)

Drawing inspiration from architectures like LMAX, an in-memory state management approach will be prioritized. This offers significant speed advantages, which are paramount for high-frequency trading environments. For 24/7 crypto exchanges, any system downtime is highly problematic. Therefore, prioritizing in-memory operations while strategically managing persistence (e.g., via the `afterOrder` handler for asynchronous updates) aligns with this need. [2]

Furthermore, the impact of network latency, particularly when physical servers are distant from users, necessitates careful consideration of transaction size. Efforts will be made to minimize the size of each transaction payload and optimize the number of parameters, potentially leveraging efficient binary serialization protocols like Protobuf, to reduce network transfer costs. This optimization also extends to in-memory data structures, where minimizing the memory footprint of core types (e.g., "Order" type) will be considered, similar to how smart contract developers optimize data structures for efficient use within a fixed EVM word size.

## References

[1] https://b2broker.com/news/what-is-cryptocurrency-matching-engine/
[2] https://martinfowler.com/articles/lmax.html#KeepingItAllInMemory

## More

*   **Minimal LMAX Concept Implementation (Python):** https://github.com/quantyle/matching-engine
